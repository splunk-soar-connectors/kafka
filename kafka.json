{
    "appid": "8af3097f-fa31-47fd-bf2f-42f26f2ccc7e",
    "name": "Kafka",
    "description": "This app implements ingesting and sending data on the Apache Kafka messaging system",
    "publisher": "Splunk",
    "package_name": "phantom_kafka",
    "type": "devops",
    "license": "Copyright (c) 2017-2026 Splunk Inc.",
    "main_module": "kafka_connector.py",
    "app_version": "2.0.8",
    "utctime_updated": "2024-09-30T13:04:00.000000Z",
    "product_vendor": "Apache",
    "product_name": "Kafka",
    "product_version_regex": ".*",
    "min_phantom_version": "6.3.0",
    "fips_compliant": true,
    "logo": "logo_apache.svg",
    "logo_dark": "logo_apache_dark.svg",
    "python_version": "3.9, 3.13",
    "configuration": {
        "hosts": {
            "data_type": "string",
            "description": "Hosts in the cluster (comma separated e.g. host1.com:9092,host2.org:2181,10.10.10.10:9092)",
            "required": true,
            "order": 0
        },
        "topic": {
            "data_type": "string",
            "description": "Topic to subscribe to for ingestion",
            "required": true,
            "order": 1
        },
        "message_parser": {
            "data_type": "file",
            "description": "Python file containing a message parsing method",
            "required": false,
            "order": 2
        },
        "ph": {
            "data_type": "ph",
            "order": 3
        },
        "timeout": {
            "data_type": "numeric",
            "description": "How long to poll for messages each interval (ms)",
            "required": true,
            "order": 4
        },
        "read_from_beginning": {
            "data_type": "boolean",
            "description": "Start ingesting from the beginning of the topic",
            "required": true,
            "default": false,
            "order": 5
        },
        "use_kerberos": {
            "data_type": "boolean",
            "description": "Use Kerberos auth",
            "required": false,
            "default": false,
            "order": 6
        },
        "use_ssl": {
            "data_type": "boolean",
            "description": "Use SSL",
            "required": false,
            "default": false,
            "order": 7
        },
        "cert_file": {
            "data_type": "string",
            "description": "Path to SSL certificate file",
            "required": false,
            "order": 8
        },
        "key_file": {
            "data_type": "string",
            "description": "Path to SSL key file",
            "required": false,
            "order": 9
        },
        "ca_cert": {
            "data_type": "string",
            "description": "Path to CA certificate file",
            "required": false,
            "order": 10
        }
    },
    "actions": [
        {
            "action": "test connectivity",
            "description": "Checks connectivity with configured hosts",
            "type": "test",
            "identifier": "test_connectivity",
            "read_only": true,
            "parameters": {},
            "output": [],
            "versions": "EQ(*)"
        },
        {
            "action": "on poll",
            "description": "Ingest messages from Kafka",
            "verbose": "Basic configuration parameters for this action are available in asset configuration.<br><br>If <b>read_from_beginning</b> is set to true, the first poll will begin reading messages from the start of a topic, ingesting as many messages as can be ingested within the time set by the <b>timeout</b> asset configuration parameter. For a <b>poll now</b>, the app will ingest as many messages as specified by <b>artifact_count</b>, starting at the beginning of the topic.<br><br>This app creates containers and artifacts using the same format as the REST endpoint. It uses a message parsing method to decide how the containers and artifacts will look. A custom message parsing script can be uploaded during asset configuration to change how Kafka messages are ingested as containers and artifacts. There are three requirements for this script:<ul><li>It must contain a method called <b>parse_messages</b>. This is the method that will be called during the poll.</li><li>The method must accept exactly two arguments:<ul><li>First argument: A string which will be the name of the topic.</li><li>Second argument: A list of dictionaries, with a dictionary containing data for each ingested message. Each dictionary will have 3 fields: <b>message</b>, <b>offset</b>, and <b>partition</b>.</li></ul><li>The method must return a list of dictionaries. Each dictionary must contain two fields:</li><ul><li><b>container</b> - a dictionary formatted as the body of a REST call to the Phantom /rest/container endpoint.</li><li><b>artifacts</b> - a list of dictionaries, with each dictionary formatted as the body of a REST call to the Phantom /rest/artifact endpoint.</li></ul></ul>Refer to the <b>REST API Documentation</b> section of the Phantom docs for more information on what can be included in the dictionaries used to create the containers and artifacts during ingestion.<br><br>The default message parsing script is called <b>kafka_parser.py</b> and can be found in the Kafka app directory. It contains:<br><br><pre>from datetime import datetime<br>time_format = '%Y-%m-%d %H:%M:%S'<br><br><br>def parse_messages(topic, messages):<br><br>    ret_json = {}<br>    container_json = {}<br>    artifact_list = []<br><br>    ret_json['container'] = container_json<br>    ret_json['artifacts'] = artifact_list<br><br>    name = 'Messages ingested from {0} at {1}'.format(topic, datetime.now().strftime(time_format))<br><br>    container_json['name'] = name<br>    container_json['description'] = 'Some messages from Kafka'<br>    container_json['run_automation'] = False<br><br>    count = 0 <br>    num_artifacts = len(messages)<br>    for message in messages:<br><br>        artifact_json = {}<br>        artifact_list.append(artifact_json)<br><br>        artifact_json['source_data_identifier'] = '{0}:{1}'.format(message['partition'], message['offset'])<br>        artifact_json['cef'] = {'message': message['message']}<br>        artifact_json['name'] = message['message'][:100]<br><br>        if count < num_artifacts - 1:<br>            artifact_json['run_automation'] = False<br><br>        count += 1<br><br>    return [ret_json]</pre><br> This script will create one container per poll, and create one artifact in that container per message ingested. The source data identifier for the artifacts will have the format <code>&lt;partition&gt;:&lt;offset&gt;</code> for each message. The data from the message will be put in the <b>message</b> CEF field. Any formatting of the data will not be preserved. Supply a custom parser to format the data as needed.",
            "type": "ingest",
            "identifier": "on_poll",
            "read_only": false,
            "parameters": {
                "start_time": {
                    "data_type": "numeric",
                    "description": "Parameter ignored in this app"
                },
                "end_time": {
                    "data_type": "numeric",
                    "description": "Parameter ignored in this app"
                },
                "container_id": {
                    "data_type": "string",
                    "description": "Parameter ignored in this app"
                },
                "container_count": {
                    "data_type": "numeric",
                    "description": "Parameter ignored in this app"
                },
                "artifact_count": {
                    "data_type": "numeric",
                    "description": "Maximum number of messages to ingest during poll now",
                    "required": false,
                    "default": 1
                }
            },
            "output": [],
            "versions": "EQ(*)"
        },
        {
            "action": "post data",
            "description": "Post data to a Kafka topic",
            "verbose": "This action creates a short-lived Kafka Producer that will post the supplied data to the given topic, then exit.<br><br>Two types of data are supported right now: string and JSON. When sending JSON, the app will format the JSON before posting it to the Kafka server. If the JSON is a list, the app will send each element of the list in separate messages. To send a list to Kafka, add an extra set of brackets around the list.<br><br>If the <b>timeout</b> parameter is set to 0 (which is the default), the app will not wait for the Kafka server to acknowledge receipt of the message. In such a scenario, the action will not fill the <b>action_result.data.*</b> data paths in the result.",
            "type": "generic",
            "identifier": "post_data",
            "read_only": false,
            "lock": {
                "enabled": false
            },
            "parameters": {
                "data_type": {
                    "description": "The type of the data being posted (can be string or JSON)",
                    "primary": false,
                    "data_type": "string",
                    "value_list": [
                        "string",
                        "JSON"
                    ],
                    "default": "string",
                    "order": 0,
                    "required": true
                },
                "data": {
                    "description": "The data to post",
                    "primary": false,
                    "data_type": "string",
                    "order": 1,
                    "required": true
                },
                "topic": {
                    "description": "The topic to post the data to",
                    "primary": true,
                    "data_type": "string",
                    "required": true,
                    "order": 2,
                    "contains": [
                        "kafka topic"
                    ]
                },
                "timeout": {
                    "description": "How long (in seconds) to wait for message to be acknowledged by server",
                    "primary": false,
                    "data_type": "numeric",
                    "required": false,
                    "order": 3,
                    "default": 0
                }
            },
            "render": {
                "width": 12,
                "title": "Post Data",
                "type": "table",
                "height": 5
            },
            "output": [
                {
                    "data_path": "action_result.status",
                    "data_type": "string",
                    "column_name": "Status",
                    "column_order": 2
                },
                {
                    "data_path": "action_result.parameter.data",
                    "data_type": "string"
                },
                {
                    "data_path": "action_result.parameter.data_type",
                    "data_type": "string",
                    "column_name": "Data Type",
                    "column_order": 1
                },
                {
                    "data_path": "action_result.parameter.timeout",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.parameter.topic",
                    "data_type": "string",
                    "column_name": "Topic",
                    "column_order": 0,
                    "contains": [
                        "kafka topic"
                    ]
                },
                {
                    "data_path": "action_result.data.*.checksum",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.offset",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.partition",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.serialized_key_size",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.serialized_value_size",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.timestamp",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.data.*.topic",
                    "data_type": "string",
                    "contains": [
                        "kafka topic"
                    ]
                },
                {
                    "data_path": "action_result.data.*.topic_partition",
                    "data_type": "numeric"
                },
                {
                    "data_path": "action_result.summary",
                    "data_type": "string"
                },
                {
                    "data_path": "action_result.message",
                    "data_type": "string"
                },
                {
                    "data_path": "summary.total_objects",
                    "data_type": "numeric"
                },
                {
                    "data_path": "summary.total_objects_successful",
                    "data_type": "numeric"
                }
            ],
            "versions": "EQ(*)"
        }
    ],
    "pip313_dependencies": {
        "wheel": [
            {
                "module": "decorator",
                "input_file": "wheels/py3/decorator-5.1.1-py3-none-any.whl"
            },
            {
                "module": "gssapi",
                "input_file": "wheels/py313/gssapi-1.10.1-cp313-cp313-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl"
            },
            {
                "module": "kafka_python",
                "input_file": "wheels/shared/kafka_python-2.3.0-py2.py3-none-any.whl"
            }
        ]
    },
    "pip39_dependencies": {
        "wheel": [
            {
                "module": "decorator",
                "input_file": "wheels/py3/decorator-5.1.1-py3-none-any.whl"
            },
            {
                "module": "gssapi",
                "input_file": "wheels/py39/gssapi-1.7.3-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl"
            },
            {
                "module": "kafka_python",
                "input_file": "wheels/shared/kafka_python-2.3.0-py2.py3-none-any.whl"
            }
        ]
    }
}
